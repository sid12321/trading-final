# @package _global_

defaults:
  - /agent/ppo
  - _self_

# Note: we do not use reward_scaling=10.0 in playground

normalize_obs: true
normalize_gae: true
num_envs: 2048
minibatch_size: 30720 # batch_size*unroll_length in brax = 1024*30
rollout_length: 480  # unroll_length*(batch_size * num_minibatches // num_envs) in brax = 30*(1024*32//2048)
reuse_rollout_epochs: 16

discount: 0.995
clip_epsilon: 0.3

total_timesteps: 60_000_000

loss_weights:
  actor_loss: 1.0
  critic_loss: 0.5
  actor_entropy: -0.01

optimizer:
  lr: 0.001
  grad_clip_norm: 0 # set 0 or null to turn-off

agent_network:
  actor_hidden_layer_sizes: [128, 128, 128, 128]
  critic_hidden_layer_sizes: [256, 256, 256, 256, 256]

num_eval_envs: 128
eval_episodes: 128 # should be divided by num_eval_envs
eval_interval: 5
