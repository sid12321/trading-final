# @package _global_

defaults:
  - /agent/ppo
  - _self_

normalize_obs: true
num_envs: 2048
minibatch_size: 12800  # 50*256
rollout_length: 200  # 50*(256*32//2048)
reuse_rollout_epochs: 8
discount: 0.95

total_timesteps: 50_000_000

loss_weights:
  actor_entropy: -0.001

num_eval_envs: 128
eval_episodes: 128 # should be divided by num_eval_envs
eval_interval: 5
