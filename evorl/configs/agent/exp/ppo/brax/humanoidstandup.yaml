# @package _global_

defaults:
  - /agent/ppo
  - _self_

normalize_obs: true
num_envs: 2048
minibatch_size: 15360 # 1024*15
rollout_length: 240 # 15*(1024*32//2048)
reuse_rollout_epochs: 8
discount: 0.97

total_timesteps: 100_000_000

loss_weights:
  actor_entropy: -0.001

optimizer:
  lr: 0.0006

num_eval_envs: 128
eval_episodes: 128 # should be divided by num_eval_envs
eval_interval: 5
