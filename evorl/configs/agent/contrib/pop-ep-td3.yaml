# @package _global_

workflow_cls: evorl.algorithms.contrib.pop_episodic_td3.PopEpisodicTD3Workflow

num_envs: 1
normalize_obs: false
discount: 0.99
total_episodes: 10000 # iters = total_episodes / (episodes_for_fitness * pop_size), e.g: 3125 iters here; not include random_timesteps.

random_timesteps: 25600 # steps filled into the replay buffer
batch_size: 256
replay_buffer_capacity: 1000000

tau: 0.005 # soft update rate
exploration_epsilon: 0.1
policy_noise: 0.2
clip_policy_noise: 0.5
actor_update_interval: 2 # use default TD3 setting
critics_in_actor_loss: first # first or min

pop_size: 8
num_learning_offspring: 8 # number of offspring to learn from RL(TD3)

fitness_with_exploration: true
episodes_for_fitness: 1 # must be devided by num_envs
num_rl_updates_per_iter: 256 # num of RL updates per iter

optimizer:
  lr: 0.0003
  grad_clip_norm: 0 # set 0 or none to turn-off

agent_network:
  norm_layer_type: "layer_norm"
  num_critics: 2
  critic_hidden_layer_sizes: [256, 256]
  actor_hidden_layer_sizes: [256, 256]

num_eval_envs: 128
eval_episodes: 128 # should be divided by num_eval_envs
eval_interval: 10

save_replay_buffer: true
