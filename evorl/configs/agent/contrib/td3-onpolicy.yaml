# @package _global_

workflow_cls: evorl.algorithms.contrib.td3_onpolicy.TD3OnPolicyWorkflow

num_envs: 256
normalize_obs: false
rollout_length: 20
discount: 0.99
total_timesteps: 50000000

minibatch_size: 256 # num_minibatches = batch_size / minibatch_size = 8
reuse_rollout_epochs: 16 # apply multiple epochs (one epoch contains multiple minibatches) on the same rollout data

num_eval_envs: 16
eval_episodes: 16 # should be divided by num_eval_envs
eval_interval: 50 # unit: num of iterations

tau: 0.005 # soft update rate
exploration_epsilon: 0.1
policy_noise: 0.2
clip_policy_noise: 0.5
actor_update_interval: 2
num_updates_per_iter: 1 # #updates per rollout, usually use: num_envs * rollout_length // actor_update_interval
critics_in_actor_loss: first # first or min

optimizer:
  lr: 0.0003
  grad_clip_norm: 0 # set 0 or none to turn-off

agent_network:
  norm_layer_type: "none"
  num_critics: 2
  critic_hidden_layer_sizes: [256, 256]
  actor_hidden_layer_sizes: [256, 256]

save_replay_buffer: true
