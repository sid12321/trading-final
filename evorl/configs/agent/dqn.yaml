# @package _global_

workflow_cls: evorl.algorithms.dqn.DQNWorkflow

num_envs: 1
normalize_obs: false
rollout_length: 1
discount: 0.99
total_timesteps: 500000
fold_iters: 64
num_updates_per_iter: 1

num_eval_envs: 16
eval_episodes: 16 # should be divided by num_eval_envs
eval_interval: 64

random_timesteps: 4096 # steps filled into the replay buffer
learning_start_timesteps: 4096 # steps before training starts
batch_size: 128
replay_buffer_capacity: 10000

exploration_epsilon:
  start: 0.8
  end: 0.05
  exploration_fraction: 0.5 # total steps: total_updates * exploration_fraction

tau: 1.0
target_network_update_freq: 100
target_type: DQN

optimizer:
  lr: 0.0003
  grad_clip_norm: null # disabled

agent_network:
  q_hidden_layer_sizes: [256, 256]
  value_obs_key: null

save_replay_buffer: true
