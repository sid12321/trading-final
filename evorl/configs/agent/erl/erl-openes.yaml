# @package _global_

workflow_cls: evorl.algorithms.erl.erl_td3.erl_eda.ERLEDAWorkflow

num_envs: 1
normalize_obs: false
discount: 0.99
total_episodes: 100000 # iters = total_episodes / (episodes_for_fitness * pop_size), e.g: 3125 iters here; not include random_timesteps.

random_timesteps: 0 # random steps filled into the replay buffer
replay_buffer_capacity: 1000000

# ====== TD3 Params ======
tau: 0.005 # soft update rate
exploration_epsilon: 0.1
policy_noise: 0.2
clip_policy_noise: 0.5
actor_update_interval: 2
critics_in_actor_loss: first # first or min

# ====== RL Train Params ======
warmup_iters: 10 # steps that only learn by EC
num_rl_updates_per_iter: 4096

episodes_for_fitness: 1 # must be devided by num_envs
rollout_episodes: 1 # must be devided by num_envs
fitness_with_exploration: false # for EC Agent
rl_exploration: true # for RL Agent

batch_size: 256

optimizer:
  lr: 0.0003
  grad_clip_norm: 0 # set 0 or none to turn-off

rl_injection_interval: 1 # replace worst pop agents by rl agents
rl_injection_stepsize: 1.0 # 1: replaced by rl weights, 0: do not inject

# ====== GA Params ======
pop_size: 32
mirror_sampling: true

ec_noise_std:
  init: 0.03
  final: 0.001
  decay: 1.0 # 1.0 means: always use fixed init value
ec_lr:
  init: 0.05
  final: 0.001
  decay: 1.0 # 1.0 means: always use fixed init value

# ================

agent_network:
  norm_layer_type: "none"
  num_critics: 2
  critic_hidden_layer_sizes: [256, 256]
  actor_hidden_layer_sizes: [256, 256]

num_eval_envs: 128
eval_episodes: 128 # should be divided by num_eval_envs
eval_interval: 10

save_replay_buffer: true
