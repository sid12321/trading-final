# @package _global_

workflow_cls: evorl.algorithms.erl.cemrl_td3.cemrl.CEMRLWorkflow

num_envs: 1
normalize_obs: false
discount: 0.99
total_episodes: 100000 # iters = total_episodes / (episodes_for_fitness * pop_size), e.g: 3125 iters here; not include random_timesteps.

random_timesteps: 0 # steps filled into the replay buffer
batch_size: 256
replay_buffer_capacity: 1000000

tau: 0.005 # soft update rate
exploration_epsilon: 0.1
policy_noise: 0.2
clip_policy_noise: 0.5
actor_update_interval: 2
critics_in_actor_loss: first # first or min


# cem hyperparams:
warmup_iters: 100 # steps that only learn by CEM

pop_size: 32
num_learning_offspring: 16 # number of offspring to learn from RL(TD3)

num_elites: 8
cov_eps:
  init: 1e-2
  final: 1e-5
  decay: 0.001 # Polyak averaging step-size, 0.001 is suitable for 1000 iters
weighted_update: true
rank_weight_shift: 1.0 # CEM-RL use 1.0; CMA-ES use 0.5; no significant diff when num_elites is large
mirror_sampling: false

fitness_with_exploration: false
episodes_for_fitness: 1 # must be devided by num_envs
num_rl_updates_per_iter: 256 # num of RL updates per iter

optimizer:
  lr: 0.0003
  grad_clip_norm: 0 # set 0 or none to turn-off

agent_network:
  norm_layer_type: "layer_norm"
  num_critics: 2
  critic_hidden_layer_sizes: [256, 256]
  actor_hidden_layer_sizes: [256, 256]

num_eval_envs: 128
eval_episodes: 128 # should be divided by num_eval_envs
eval_interval: 10

save_replay_buffer: true
